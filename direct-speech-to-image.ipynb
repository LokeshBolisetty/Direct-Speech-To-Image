{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Direct Speech To Text","metadata":{}},{"cell_type":"markdown","source":"Please run this file in Kaggle and import the [dataset](https://www.kaggle.com/datasets/lokeshbolisetty/speech-to-image-dataset).\n\nGithub repo can be found [here](https://github.com/LokeshBolisetty/Direct-Speech-To-Image)","metadata":{}},{"cell_type":"markdown","source":"### IMPORTING REQUIRED LIBRARIES","metadata":{}},{"cell_type":"code","source":"import librosa\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython import display\nfrom IPython.display import clear_output\n\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\ntorch.manual_seed(0)\nfrom torchvision.utils import make_grid\nfrom torchvision.utils import save_image\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm.notebook import tqdm\nimport statistics\n\nimport IPython.display as ipd\nfrom IPython.core.display import display\nfrom IPython.display import Image\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:10:35.215033Z","iopub.execute_input":"2022-05-16T14:10:35.215682Z","iopub.status.idle":"2022-05-16T14:10:39.652243Z","shell.execute_reply.started":"2022-05-16T14:10:35.215554Z","shell.execute_reply":"2022-05-16T14:10:39.651416Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Making the requirements file\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:10:39.654174Z","iopub.execute_input":"2022-05-16T14:10:39.654422Z","iopub.status.idle":"2022-05-16T14:10:44.559227Z","shell.execute_reply.started":"2022-05-16T14:10:39.654388Z","shell.execute_reply":"2022-05-16T14:10:44.558039Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:10:44.561935Z","iopub.execute_input":"2022-05-16T14:10:44.562282Z","iopub.status.idle":"2022-05-16T14:10:44.627192Z","shell.execute_reply.started":"2022-05-16T14:10:44.562238Z","shell.execute_reply":"2022-05-16T14:10:44.626087Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Add dataset from [here](https://www.kaggle.com/datasets/lokeshbolisetty/speech-to-image-dataset)","metadata":{}},{"cell_type":"markdown","source":"# Speech captions generation","metadata":{}},{"cell_type":"markdown","source":"We are using Google Text to speech(referred to gtts from hereon) software to generate audio captions since the dataset only has text captions. However, there is a limit on the number of requests you can send to that API. Therefore we had to delay the generation and as a result, the code takes very very long time to run(>12 hrs). Anyway, it is not really feasible computationally to work on this entire data. So we sampled some of this data randomly and added it to our [dataset](https://www.kaggle.com/datasets/lokeshbolisetty/speech-to-image-dataset). It is recommended to use that and not to run this. However, the code to generate this data can be found [here](https://github.com/LokeshBolisetty/DL-Project/tree/main/TTS). ","metadata":{}},{"cell_type":"markdown","source":"# AUDIO ENCODING","metadata":{}},{"cell_type":"markdown","source":"### PREPROCESSING DATA FOR AUDIO ENCODER","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '../input/speech-to-image-dataset/Sound'\nTRAIN_PATH = BASE_PATH  \n\n# Sampling rate per sec\nsr = 22050\n# Load duration- 2s\nload_duration = 2\n# Samples for corresponding load duration\nTOTAL_SAMPLES = load_duration * sr\n# Number of slices to be created of the load duration\nNUM_SLICES = 1\n\nSAMPLES_PER_SLICE = int(TOTAL_SAMPLES / NUM_SLICES)\nDATA_POINTS = TOTAL_SAMPLES","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:10:44.629543Z","iopub.execute_input":"2022-05-16T14:10:44.630238Z","iopub.status.idle":"2022-05-16T14:10:44.638402Z","shell.execute_reply.started":"2022-05-16T14:10:44.630191Z","shell.execute_reply":"2022-05-16T14:10:44.637482Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The project needs audio inputs but the captions are in text form. Since they are not available, we are using Google Text to Speech to convert the text into speech. The length of each speech sample is varied. However, we need them to be of the same size to give them as inputs to the MLP. Therefore, we add padding to the array to make it of length ```22050*5``` because ```22050``` is the sampling rate per sec and 5 is the maximum length of speech we are considering(We observed that most audio samples have a length approx 4 sec and adding more length than required using padding will only decrease the amount of true data). ","metadata":{}},{"cell_type":"markdown","source":"This is computationally expensive because we need to load a lot of samples. But using the entire dataset is not computationally feasible here, so we are using only some part of the data. Decreasing the sampling rate results in bad accuracy and hence we are going with decreased number of samples. This is a decision after trying different sizes of datasets. ","metadata":{}},{"cell_type":"code","source":"# Loading the audio dataset for cats and dogs\n\n#We are ignoring warnings here because everytime we load a file, we get a warning saying pySoundRead failed and that it is using a different function. \n#But since the warning is from inside librosa, there isnt much we can do about it and it is not a problem because the file runs perfectly with it\n#This is happening because the wav format gTTS gives is not the exact wav format that librosa uses. However, this is nothing to worry about. \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#This holds the values of the pre-trained input audio\nx_train_pre = []\n#This holds whether the input is corresponding to a cat or a dog\nlabels = []\n\n#The path has two folders Cats adn Dogs. Each of them considered\n#This takes a couple of minutes to run because we are appending mean value to the numpy arrays and there are a lot of files\nfor i, class_type in enumerate(sorted(os.listdir(TRAIN_PATH))):\n    print(class_type+\" processing has started\")\n    animal_list = np.array(sorted(os.listdir(TRAIN_PATH+'/'+class_type)))\n\n    for animal in animal_list:\n        filename = TRAIN_PATH+'/'+class_type+'/'+animal\n        audio, sr = librosa.load(filename, offset=0.0, duration=load_duration) #Load timeseries data into audio using librosa. The sampling rate is default value, 22050, load duration is 5 sec\n        \n        meanValue = audio.sum()/len(audio) #Finding the mean value of audio\n\n        while(audio.size!=sr*load_duration): #Forcing audio to be of same length in every row\n            audio = np.append(audio, meanValue) #by appending mean value if the length is lesser\n        x_train_pre.append(audio[:SAMPLES_PER_SLICE]) #If the length is larger than 5sec, we just ignore the additional portion. \n\n        #Making the labels\n        if(class_type=='Cats'):\n          labels.append(0) #considering 0s represent Cats and \n        else:\n          labels.append(1) #1s represent Dogs\n\n#Converting the lists to numpy arrays for further use\nx_train_pre = np.array(x_train_pre) \nlabels = np.array(labels)\n\n#This will take around 10 minutes to load","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:10:44.642301Z","iopub.execute_input":"2022-05-16T14:10:44.642636Z","iopub.status.idle":"2022-05-16T14:13:03.172454Z","shell.execute_reply.started":"2022-05-16T14:10:44.642566Z","shell.execute_reply":"2022-05-16T14:13:03.171444Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Checking the shape of x_train_pre to make sure that the length of each row is the same (22050*5)\nprint(x_train_pre.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.174763Z","iopub.execute_input":"2022-05-16T14:13:03.175095Z","iopub.status.idle":"2022-05-16T14:13:03.181145Z","shell.execute_reply.started":"2022-05-16T14:13:03.175053Z","shell.execute_reply":"2022-05-16T14:13:03.179954Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Total samples in an audio file is sampling rate per sec times number of seconds\nDATA_POINTS = x_train_pre.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.182803Z","iopub.execute_input":"2022-05-16T14:13:03.183441Z","iopub.status.idle":"2022-05-16T14:13:03.190207Z","shell.execute_reply.started":"2022-05-16T14:13:03.183399Z","shell.execute_reply":"2022-05-16T14:13:03.189384Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of x_train_pre: {x_train_pre.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.192098Z","iopub.execute_input":"2022-05-16T14:13:03.192693Z","iopub.status.idle":"2022-05-16T14:13:03.199322Z","shell.execute_reply.started":"2022-05-16T14:13:03.192652Z","shell.execute_reply":"2022-05-16T14:13:03.198441Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Showing a sample audio clip\nipd.Audio(x_train_pre[300],rate=22050)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.201152Z","iopub.execute_input":"2022-05-16T14:13:03.201968Z","iopub.status.idle":"2022-05-16T14:13:03.219005Z","shell.execute_reply.started":"2022-05-16T14:13:03.201926Z","shell.execute_reply":"2022-05-16T14:13:03.218281Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"The audio heard earlier belongs to class \", labels[300])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.219904Z","iopub.execute_input":"2022-05-16T14:13:03.220158Z","iopub.status.idle":"2022-05-16T14:13:03.225966Z","shell.execute_reply.started":"2022-05-16T14:13:03.220124Z","shell.execute_reply":"2022-05-16T14:13:03.225105Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### DATASET AND DATA LOADER FOR AUDIO ENCODER","metadata":{}},{"cell_type":"code","source":"#Using a batch size of 20\nbatch_size = 20","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-16T14:13:03.227631Z","iopub.execute_input":"2022-05-16T14:13:03.228763Z","iopub.status.idle":"2022-05-16T14:13:03.235748Z","shell.execute_reply.started":"2022-05-16T14:13:03.228695Z","shell.execute_reply":"2022-05-16T14:13:03.234892Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Making the dataset\nfrom torch.utils.data import DataLoader, Dataset\nkwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n\nclass AnimalDataset(Dataset):\n    def __init__(self, animal_data):\n        self.animal_data = animal_data\n\n    def __len__(self):\n        return len(self.animal_data)\n\n    #Method to get each item\n    def __getitem__(self, idx):\n        data = self.animal_data[idx]\n        data = torch.from_numpy(data)\n        return (data,idx) #Returning the index so that we know which class the given audio belongs to later using this index and labels array\n\n#Making the train test split in 70-30\ntrain_size = int(0.7*x_train_pre.shape[0])\ntest_size = x_train_pre.shape[0]-train_size\n\n#Splitting the data randomly\ntrain_set, test_set = torch.utils.data.random_split(AnimalDataset(x_train_pre), [train_size, test_size], generator=torch.Generator().manual_seed(401))\n\n#Making the train and test loader\ndata_train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\ndata_test_loader = DataLoader(test_set, batch_size = batch_size, shuffle=True, **kwargs)\nprint(\"The train set is \",len(train_set),\"long and the test set is \",len(test_set),\" long\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.237418Z","iopub.execute_input":"2022-05-16T14:13:03.238050Z","iopub.status.idle":"2022-05-16T14:13:03.254147Z","shell.execute_reply.started":"2022-05-16T14:13:03.238000Z","shell.execute_reply":"2022-05-16T14:13:03.253158Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### MODEL OF AUDIO ENCODER","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# model architecture\n\nclass Encoder(nn.Module):\n    def __init__(self, featureDim=DATA_POINTS):\n        super(Encoder, self).__init__()\n\n        #Creating a vanilla MLP\n        self.fc1 = nn.Linear(featureDim, 16384)\n        self.fc2 = nn.Linear(16384, 4096)\n        self.fc3 = nn.Linear(4096, 2048)\n        self.fc4 = nn.Linear(2048, 512)\n        self.fc5 = nn.Linear(512, 128)\n        self.fc6 = nn.Linear(128, 32)\n        self.fc7 = nn.Linear(32, 8)\n        self.fc8 = nn.Linear(8,1)\n        self.act = nn.LeakyReLU(0.1)\n\n    def encoder(self, x):\n        x = self.act(self.fc1(x))\n        x = self.act(self.fc2(x))\n        x = self.act(self.fc3(x))\n        x = self.act(self.fc4(x))\n        x = self.act(self.fc5(x))\n        x = self.act(self.fc6(x))\n        x = self.act(self.fc7(x))\n        x = torch.sigmoid(self.fc8(x))\n        return x\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.255922Z","iopub.execute_input":"2022-05-16T14:13:03.256353Z","iopub.status.idle":"2022-05-16T14:13:03.267247Z","shell.execute_reply.started":"2022-05-16T14:13:03.256313Z","shell.execute_reply":"2022-05-16T14:13:03.266127Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### LOSS FUNCTION FOR AUDIO ENCODER","metadata":{}},{"cell_type":"code","source":"#Using binary cross entropy loss because we are doing a binary classification problem\ndef loss_function(y_, y):\n    BCE = nn.BCELoss(y_, y)\n    return BCE","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.272933Z","iopub.execute_input":"2022-05-16T14:13:03.273146Z","iopub.status.idle":"2022-05-16T14:13:03.279785Z","shell.execute_reply.started":"2022-05-16T14:13:03.273121Z","shell.execute_reply":"2022-05-16T14:13:03.278840Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### TRAINING AUDIO ENCODER","metadata":{}},{"cell_type":"code","source":"# model instantiation\nmodel = Encoder()\nmodel.to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:03.281217Z","iopub.execute_input":"2022-05-16T14:13:03.281695Z","iopub.status.idle":"2022-05-16T14:13:14.428419Z","shell.execute_reply.started":"2022-05-16T14:13:03.281653Z","shell.execute_reply":"2022-05-16T14:13:14.427561Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Optimizer\n#After testing a couple of options for lr, we realised that lr should be as large as 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:14.430006Z","iopub.execute_input":"2022-05-16T14:13:14.430510Z","iopub.status.idle":"2022-05-16T14:13:14.435487Z","shell.execute_reply.started":"2022-05-16T14:13:14.430463Z","shell.execute_reply":"2022-05-16T14:13:14.434578Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"epochs = 160\nloss_values = []\n#labels = torch.from_numpy(labels).float().to(device)\nfor epoch in range(1, epochs+1):\n    # monitor training loss\n    train_loss = 0.0\n\n    #Training\n    for data in data_train_loader:\n        #Extracting the actual label\n        label = data[1]\n        data = data[0]\n\n        data = data.to(device).float()\n\n        #Making the tensor for the labels of all samples in this batch\n        thisLabel = torch.Tensor(labels[label])\n        thisLabel = thisLabel.to(device)\n        optimizer.zero_grad()\n        outputs = model(data)\n\n        try:\n            outputs = outputs.reshape(batch_size) #For all the batches\n        except:\n            outputs = outputs.reshape(train_size % batch_size) #For the last batch\n\n        loss = torch.nn.BCELoss()\n        thisLoss = loss(outputs, thisLabel)\n        thisLoss.backward()\n        optimizer.step()\n        train_loss += thisLoss.item()*data.size(0)\n\n    train_loss = train_loss/len(data_train_loader)\n    loss_values.append(train_loss)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:13:14.436947Z","iopub.execute_input":"2022-05-16T14:13:14.437402Z","iopub.status.idle":"2022-05-16T14:17:34.801604Z","shell.execute_reply.started":"2022-05-16T14:13:14.437360Z","shell.execute_reply":"2022-05-16T14:17:34.800726Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Accuracy check\n\n#Holds the number of correct predictions\ncorrect = 0\n\nfor data in data_test_loader:\n    label = data[1] #Loading the actual labels\n\n    data = data[0] #Data is the audio file's numpy array\n    data = data.to(device) #Sending data to GPU\n\n    #thisLabel is the labels of all the elements in the current batch\n    thisLabel = torch.Tensor(labels[label])\n    thisLabel = thisLabel.to(device)\n    outputs = model(data)\n\n    try:\n        outputs = outputs.reshape(batch_size) #For all the batches\n    except:\n        outputs = outputs.reshape(test_size % batch_size) #For the last batch\n    \n    #Counting the number of correct outputs\n    i=0\n    for x in thisLabel:\n      #Setting the threshold as 0.5\n      if(outputs[i]<0.5):\n        outputs[i] = 0\n      else:\n        outputs[i] = 1\n      #If the outputs match, increase correct\n      if(x==outputs[i]):\n        correct = correct+1\n      i = i+1\n    \nprint(\"The accuracy on test data is \",correct/test_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:17:34.803293Z","iopub.execute_input":"2022-05-16T14:17:34.803783Z","iopub.status.idle":"2022-05-16T14:17:34.928937Z","shell.execute_reply.started":"2022-05-16T14:17:34.803740Z","shell.execute_reply":"2022-05-16T14:17:34.928036Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.array(loss_values), color=\"blue\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid('True')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:17:34.930574Z","iopub.execute_input":"2022-05-16T14:17:34.930944Z","iopub.status.idle":"2022-05-16T14:17:35.185528Z","shell.execute_reply.started":"2022-05-16T14:17:34.930895Z","shell.execute_reply":"2022-05-16T14:17:35.184687Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Saving the model for future use\ntorch.save(model.state_dict(),'Classify.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:17:35.187008Z","iopub.execute_input":"2022-05-16T14:17:35.187491Z","iopub.status.idle":"2022-05-16T14:17:50.873956Z","shell.execute_reply.started":"2022-05-16T14:17:35.187447Z","shell.execute_reply":"2022-05-16T14:17:50.872786Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Run this snippets only if you are using the classify.pth from above cell. Otherwise do not run them. ","metadata":{}},{"cell_type":"code","source":"model = Encoder()\nmodel.load_state_dict(torch.load('./Classify.pth'))\nmodel.eval()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:17:50.875880Z","iopub.execute_input":"2022-05-16T14:17:50.876213Z","iopub.status.idle":"2022-05-16T14:18:03.060442Z","shell.execute_reply.started":"2022-05-16T14:17:50.876171Z","shell.execute_reply":"2022-05-16T14:18:03.059603Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"audio, sr = librosa.load('../input/speech-to-image-dataset/Sound/Cats/000000032816_3.wav', offset=0.0, duration=load_duration)\nwhile(audio.size!=44100):\n  audio = np.append(audio, 0)\naudio = torch.Tensor(audio).float().to(device)\nmodel(audio)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.062581Z","iopub.execute_input":"2022-05-16T14:18:03.062908Z","iopub.status.idle":"2022-05-16T14:18:03.679664Z","shell.execute_reply.started":"2022-05-16T14:18:03.062858Z","shell.execute_reply":"2022-05-16T14:18:03.678768Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# GAN","metadata":{}},{"cell_type":"code","source":"# to normalize the pixel values, we choose a mean standard deviation of 0.5 for each channel\n# this will ensure that the pixel values are in the range of (-1, 1)\n# as its very convenient to train the discriminator when the pixel values are in the range of (-1, 1)\nimage_size = 64\nbatch_size = 128 #used to create a data loader\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # means, standard deviations","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.681516Z","iopub.execute_input":"2022-05-16T14:18:03.682107Z","iopub.status.idle":"2022-05-16T14:18:03.689010Z","shell.execute_reply.started":"2022-05-16T14:18:03.682064Z","shell.execute_reply":"2022-05-16T14:18:03.688091Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# as we normalized the pixel values into (-1, 1) \n# this denormalization brings the pixel values back \n# into the range of (0, 1) we use this while we view images\ndef denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.690597Z","iopub.execute_input":"2022-05-16T14:18:03.690982Z","iopub.status.idle":"2022-05-16T14:18:03.700142Z","shell.execute_reply.started":"2022-05-16T14:18:03.690938Z","shell.execute_reply":"2022-05-16T14:18:03.699193Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# show_images takes image tensors and maximum number of images it should show and plots them in a grid\n\ndef show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\n# show_batch takes the data loader so as get the batch of images from dataloader and show the images \n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break\n\ndef show_images_2x2(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(2, 2))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.702046Z","iopub.execute_input":"2022-05-16T14:18:03.702653Z","iopub.status.idle":"2022-05-16T14:18:03.715211Z","shell.execute_reply.started":"2022-05-16T14:18:03.702578Z","shell.execute_reply":"2022-05-16T14:18:03.714223Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():   # for this to retrun true 3 conditions should hold true, \n                                    # Execution environment should be connected to a hardware which is a Nvidia GPU or a graphics card\n                                    # Cuda Drivers installed\n                                    # Pytorch version that is compatable with GPU\n                                    # all these are ensured in colab/kaggle \n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \n# to_device takes data and move it onto a target device \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.717149Z","iopub.execute_input":"2022-05-16T14:18:03.717544Z","iopub.status.idle":"2022-05-16T14:18:03.731354Z","shell.execute_reply.started":"2022-05-16T14:18:03.717506Z","shell.execute_reply":"2022-05-16T14:18:03.730344Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# this cell is just to make sure we are using gpu\n# it outputs 'cuda' in case we are using gpu, else it outputs 'cpu'\n\ndevice = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.733064Z","iopub.execute_input":"2022-05-16T14:18:03.733423Z","iopub.status.idle":"2022-05-16T14:18:03.745662Z","shell.execute_reply.started":"2022-05-16T14:18:03.733375Z","shell.execute_reply":"2022-05-16T14:18:03.744638Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"DATA_DIR_dog = '../input/speech-to-image-dataset/Photos/DogImages'\nDATA_DIR_cat = '../input/speech-to-image-dataset/Photos/CatImages'\nlatent_size = 128","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.749462Z","iopub.execute_input":"2022-05-16T14:18:03.749742Z","iopub.status.idle":"2022-05-16T14:18:03.755289Z","shell.execute_reply.started":"2022-05-16T14:18:03.749702Z","shell.execute_reply":"2022-05-16T14:18:03.754221Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"generator_dog = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True), #Activation Function\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True), #Activation Function\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True), #Activation Function\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True), #Activation Function\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 64 x 64\n)\n# So the outputs of generator are pixel values in the range of (-1, 1) and are of the shape 3*64*64\n# which is same as the Images picked from the dataset after normalization ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.757516Z","iopub.execute_input":"2022-05-16T14:18:03.758954Z","iopub.status.idle":"2022-05-16T14:18:03.814201Z","shell.execute_reply.started":"2022-05-16T14:18:03.758908Z","shell.execute_reply":"2022-05-16T14:18:03.813115Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"generator_cat = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True), #Activation Function\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True), #Activation Function\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True), #Activation Function\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True), #Activation Function\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 64 x 64\n)\n# So the outputs of generator are pixel values in the range of (-1, 1) and are of the shape 3*64*64\n# which is same as the Images picked from the dataset after normalization ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.815893Z","iopub.execute_input":"2022-05-16T14:18:03.817007Z","iopub.status.idle":"2022-05-16T14:18:03.876416Z","shell.execute_reply.started":"2022-05-16T14:18:03.816947Z","shell.execute_reply":"2022-05-16T14:18:03.875263Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.878188Z","iopub.execute_input":"2022-05-16T14:18:03.878883Z","iopub.status.idle":"2022-05-16T14:18:03.887655Z","shell.execute_reply.started":"2022-05-16T14:18:03.878833Z","shell.execute_reply":"2022-05-16T14:18:03.886211Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#cat_generator = generator_cat()\ngenerator_cat.load_state_dict(torch.load('../input/speech-to-image-dataset/SavedModels/G_cat.pth'))\ngenerator_cat.to(device)\ngenerator_cat.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:03.890217Z","iopub.execute_input":"2022-05-16T14:18:03.892157Z","iopub.status.idle":"2022-05-16T14:18:04.171403Z","shell.execute_reply.started":"2022-05-16T14:18:03.892110Z","shell.execute_reply":"2022-05-16T14:18:04.170561Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def show_one_image(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(2, 2))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:04.172983Z","iopub.execute_input":"2022-05-16T14:18:04.173264Z","iopub.status.idle":"2022-05-16T14:18:04.179342Z","shell.execute_reply.started":"2022-05-16T14:18:04.173234Z","shell.execute_reply":"2022-05-16T14:18:04.178310Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"generator_dog.load_state_dict(torch.load('../input/speech-to-image-dataset/SavedModels/G_dog.pth'))\ngenerator_dog.to(device)\ngenerator_dog.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:04.181695Z","iopub.execute_input":"2022-05-16T14:18:04.182250Z","iopub.status.idle":"2022-05-16T14:18:04.525560Z","shell.execute_reply.started":"2022-05-16T14:18:04.182205Z","shell.execute_reply":"2022-05-16T14:18:04.524582Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"xb = torch.randn(1, latent_size, 1, 1) # random latent tensors\nxb = xb.to(device)\nfake_images = generator_dog(xb)\nfake_images = fake_images.cpu()\nprint(fake_images.shape)\nshow_one_image(fake_images)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:04.528333Z","iopub.execute_input":"2022-05-16T14:18:04.529511Z","iopub.status.idle":"2022-05-16T14:18:09.619666Z","shell.execute_reply.started":"2022-05-16T14:18:04.529460Z","shell.execute_reply":"2022-05-16T14:18:09.618820Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"xb = torch.randn(1, latent_size, 1, 1) # random latent tensors\nxb = xb.to(device)\nfake_images = generator_cat(xb)\nfake_images = fake_images.cpu()\nprint(\"Image of a cat\")\nprint(fake_images.shape)\nshow_one_image(fake_images)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.621080Z","iopub.execute_input":"2022-05-16T14:18:09.621965Z","iopub.status.idle":"2022-05-16T14:18:09.726804Z","shell.execute_reply.started":"2022-05-16T14:18:09.621914Z","shell.execute_reply":"2022-05-16T14:18:09.726011Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def print_dog():\n    xb = torch.randn(1, latent_size, 1, 1) # random latent tensors\n    xb = xb.to(device)\n    fake_images = generator_dog(xb)\n    fake_images = fake_images.cpu()\n    print(\"Image of a dog: \")\n    show_one_image(fake_images)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.728171Z","iopub.execute_input":"2022-05-16T14:18:09.729014Z","iopub.status.idle":"2022-05-16T14:18:09.738478Z","shell.execute_reply.started":"2022-05-16T14:18:09.728967Z","shell.execute_reply":"2022-05-16T14:18:09.737550Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def print_cat():\n    xb = torch.randn(1, latent_size, 1, 1) # random latent tensors\n    xb = xb.to(device)\n    fake_images = generator_cat(xb)\n    fake_images = fake_images.cpu()\n    print(\"Image of a cat: \")\n    show_one_image(fake_images)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.739768Z","iopub.execute_input":"2022-05-16T14:18:09.740490Z","iopub.status.idle":"2022-05-16T14:18:09.753456Z","shell.execute_reply.started":"2022-05-16T14:18:09.740441Z","shell.execute_reply":"2022-05-16T14:18:09.752545Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Working Examples","metadata":{}},{"cell_type":"code","source":"def play_sound(tensor):\n    display(ipd.Audio(tensor,rate = sr, autoplay=True))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.754854Z","iopub.execute_input":"2022-05-16T14:18:09.755582Z","iopub.status.idle":"2022-05-16T14:18:09.766656Z","shell.execute_reply.started":"2022-05-16T14:18:09.755534Z","shell.execute_reply":"2022-05-16T14:18:09.765764Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def generateAudio(audio_path):\n    sample_audio1, sr = librosa.load(audio_path, offset=0.0, duration=load_duration)\n    while(sample_audio1.size!=44100):\n        sample_audio1 = np.append(sample_audio1, 0)\n    sample_audio1 = torch.Tensor(sample_audio1).float()\n    print(\"The said statment is \")\n    #ipd.Audio(sample_audio1,rate=22050)\n    play_sound(sample_audio1)\n    sample_audio1 = sample_audio1.to(device)\n    output = model(sample_audio1)\n    xb = torch.rand(1,latent_size, 1, 1)\n    xb = xb.to(device)\n    if(output<0.5):\n        print(\"The model categorised this as a cat.\\nNow generating Cat image\")\n        print_cat()\n    else:\n        print(\"The model categorised this as a dog. Now generating Dog image\")\n        print_dog()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.768049Z","iopub.execute_input":"2022-05-16T14:18:09.768838Z","iopub.status.idle":"2022-05-16T14:18:09.787386Z","shell.execute_reply.started":"2022-05-16T14:18:09.768793Z","shell.execute_reply":"2022-05-16T14:18:09.786488Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"audio_path = '../input/speech-to-image-dataset/Sound/Cats/000000032816_3.wav'\ngenerateAudio(audio_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:09.800625Z","iopub.execute_input":"2022-05-16T14:18:09.801233Z","iopub.status.idle":"2022-05-16T14:18:10.348647Z","shell.execute_reply.started":"2022-05-16T14:18:09.801178Z","shell.execute_reply":"2022-05-16T14:18:10.347573Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"audio_path = '../input/speech-to-image-dataset/Sound/Dogs/000000024664_0.wav'\ngenerateAudio(audio_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:10.350966Z","iopub.execute_input":"2022-05-16T14:18:10.351547Z","iopub.status.idle":"2022-05-16T14:18:10.896079Z","shell.execute_reply.started":"2022-05-16T14:18:10.351488Z","shell.execute_reply":"2022-05-16T14:18:10.895031Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Note\nRun the following code only to see that there are no errors. The following training takes about 6 hours in total. We have used the trianed model's saved states in the earlier cells. ","metadata":{}},{"cell_type":"markdown","source":"### For Dogs","metadata":{}},{"cell_type":"markdown","source":"Load the dataset using the ImageFolder class from torchvisison. Resize the images to 64x64 and normalize the pixels so that all the pixels are in the range (-1,1). ","metadata":{}},{"cell_type":"code","source":"train_ds_dog = ImageFolder(DATA_DIR_dog, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ToTensor(),\n    T.Normalize(*stats)]))\n\ntrain_dl_dog = DataLoader(train_ds_dog, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n#Ignore the warning here","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:10.898289Z","iopub.execute_input":"2022-05-16T14:18:10.899097Z","iopub.status.idle":"2022-05-16T14:18:14.788128Z","shell.execute_reply.started":"2022-05-16T14:18:10.899046Z","shell.execute_reply":"2022-05-16T14:18:14.787300Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl_dog)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:14.789487Z","iopub.execute_input":"2022-05-16T14:18:14.789934Z","iopub.status.idle":"2022-05-16T14:18:16.745596Z","shell.execute_reply.started":"2022-05-16T14:18:14.789895Z","shell.execute_reply":"2022-05-16T14:18:16.744818Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"We can now move our training data loader using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available).","metadata":{}},{"cell_type":"code","source":"# we are converting training data loader to a device data loader\ntrain_dl_dog = DeviceDataLoader(train_dl_dog, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:16.746966Z","iopub.execute_input":"2022-05-16T14:18:16.747334Z","iopub.status.idle":"2022-05-16T14:18:16.752961Z","shell.execute_reply.started":"2022-05-16T14:18:16.747299Z","shell.execute_reply":"2022-05-16T14:18:16.751947Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### For Cats","metadata":{}},{"cell_type":"code","source":"train_ds_cat = ImageFolder(DATA_DIR_cat, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ToTensor(),\n    T.Normalize(*stats)]))\n\ntrain_dl_cat = DataLoader(train_ds_cat, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n#Ignore the warning here","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:16.755038Z","iopub.execute_input":"2022-05-16T14:18:16.755395Z","iopub.status.idle":"2022-05-16T14:18:21.228233Z","shell.execute_reply.started":"2022-05-16T14:18:16.755357Z","shell.execute_reply":"2022-05-16T14:18:21.227408Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl_cat)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:21.229864Z","iopub.execute_input":"2022-05-16T14:18:21.230141Z","iopub.status.idle":"2022-05-16T14:18:24.009372Z","shell.execute_reply.started":"2022-05-16T14:18:21.230102Z","shell.execute_reply":"2022-05-16T14:18:24.008527Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_dl_cat = DeviceDataLoader(train_dl_cat, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.011465Z","iopub.execute_input":"2022-05-16T14:18:24.011992Z","iopub.status.idle":"2022-05-16T14:18:24.018588Z","shell.execute_reply.started":"2022-05-16T14:18:24.011949Z","shell.execute_reply":"2022-05-16T14:18:24.017704Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Discriminator Network","metadata":{}},{"cell_type":"code","source":"discriminator_dog = nn.Sequential(\n    # in: 3 x 64 x 64\n\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(), # to flatten it out into a single vector\n    nn.Sigmoid()) # as we have a single class we are using Sigmoid()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.020756Z","iopub.execute_input":"2022-05-16T14:18:24.021275Z","iopub.status.idle":"2022-05-16T14:18:24.062397Z","shell.execute_reply.started":"2022-05-16T14:18:24.021236Z","shell.execute_reply":"2022-05-16T14:18:24.061471Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"discriminator_cat = nn.Sequential(\n    # in: 3 x 64 x 64\n\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),#Activation Function\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True), #Activation Function\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(), # to flatten it out into a single vector\n    nn.Sigmoid()) # as we have a single class we are using Sigmoid()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.065813Z","iopub.execute_input":"2022-05-16T14:18:24.066082Z","iopub.status.idle":"2022-05-16T14:18:24.109081Z","shell.execute_reply.started":"2022-05-16T14:18:24.066051Z","shell.execute_reply":"2022-05-16T14:18:24.108148Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Note that we're using the Leaky ReLU activation for the discriminator.\n\nJust like any other binary classification model, the output of the discriminator is a single number between 0 and 1, which can be interpreted as the probability of the input image being real i.e. picked from the original dataset.","metadata":{}},{"cell_type":"code","source":"#moving the discriminator model to device\ndiscriminator_dog = to_device(discriminator_dog, device)\ndiscriminator_cat = to_device(discriminator_cat, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.112477Z","iopub.execute_input":"2022-05-16T14:18:24.112804Z","iopub.status.idle":"2022-05-16T14:18:24.127878Z","shell.execute_reply.started":"2022-05-16T14:18:24.112766Z","shell.execute_reply":"2022-05-16T14:18:24.127028Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Generator Network\n\nWe use `ConvTranspose2d` to perform *transposed convolution*. This will convert a latent tensor of (128,1,1) to (3,28,28)","metadata":{}},{"cell_type":"code","source":"latent_size = 128","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.129581Z","iopub.execute_input":"2022-05-16T14:18:24.129943Z","iopub.status.idle":"2022-05-16T14:18:24.135144Z","shell.execute_reply.started":"2022-05-16T14:18:24.129900Z","shell.execute_reply":"2022-05-16T14:18:24.133695Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"We use the TanH activation function for the output layer of the generator.","metadata":{}},{"cell_type":"code","source":"xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\nxb = xb.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.137243Z","iopub.execute_input":"2022-05-16T14:18:24.137719Z","iopub.status.idle":"2022-05-16T14:18:24.145422Z","shell.execute_reply.started":"2022-05-16T14:18:24.137670Z","shell.execute_reply":"2022-05-16T14:18:24.144310Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"fake_images_dog = generator_dog(xb)\nfake_images_dog = fake_images_dog.cpu()\nshow_images(fake_images_dog)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.147129Z","iopub.execute_input":"2022-05-16T14:18:24.148346Z","iopub.status.idle":"2022-05-16T14:18:24.613740Z","shell.execute_reply.started":"2022-05-16T14:18:24.148300Z","shell.execute_reply":"2022-05-16T14:18:24.612987Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"fake_images_cat = generator_cat(xb)\nprint(fake_images_cat.shape)\nshow_images(fake_images_cat.cpu())","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.615070Z","iopub.execute_input":"2022-05-16T14:18:24.615453Z","iopub.status.idle":"2022-05-16T14:18:24.949076Z","shell.execute_reply.started":"2022-05-16T14:18:24.615419Z","shell.execute_reply":"2022-05-16T14:18:24.945939Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# moving the generator model to the device\ngenerator_dog = to_device(generator_dog, device)\ngenerator_cat = to_device(generator_cat, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.950681Z","iopub.execute_input":"2022-05-16T14:18:24.951286Z","iopub.status.idle":"2022-05-16T14:18:24.957435Z","shell.execute_reply.started":"2022-05-16T14:18:24.951236Z","shell.execute_reply":"2022-05-16T14:18:24.956602Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def train_discriminator_dog(real_images_dog, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real images through discriminator\n    # targets are set to ones for all the real images\n    real_preds_dog = discriminator_dog(real_images_dog)\n    real_targets_dog = torch.ones(real_images_dog.size(0), 1, device=device)\n    real_loss_dog = F.binary_cross_entropy(real_preds_dog, real_targets_dog)\n    real_score_dog = torch.mean(real_preds_dog).item()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images_dog = generator_dog(latent)\n\n    # Pass fake images through discriminator\n    # targets are set to zero for all the fake images\n    fake_targets_dog = torch.zeros(fake_images_dog.size(0), 1, device=device)\n    fake_preds_dog = discriminator_dog(fake_images_dog)\n    fake_loss_dog = F.binary_cross_entropy(fake_preds_dog, fake_targets_dog)\n    fake_score_dog = torch.mean(fake_preds_dog).item()\n\n    # Update discriminator weights\n    loss_dog = real_loss_dog + fake_loss_dog\n    loss_dog.backward()\n    opt_d.step()\n    return loss_dog.item(), real_score_dog, fake_score_dog","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.959386Z","iopub.execute_input":"2022-05-16T14:18:24.959939Z","iopub.status.idle":"2022-05-16T14:18:24.971736Z","shell.execute_reply.started":"2022-05-16T14:18:24.959900Z","shell.execute_reply":"2022-05-16T14:18:24.970740Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def train_discriminator_cat(real_images_cat, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real images through discriminator\n    # targets are set to ones for all the real images\n    real_preds_cat = discriminator_cat(real_images_cat)\n    real_targets_cat = torch.ones(real_images_cat.size(0), 1, device=device)\n    real_loss_cat = F.binary_cross_entropy(real_preds_cat, real_targets_cat)\n    real_score_cat = torch.mean(real_preds_cat).item()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images_cat = generator_cat(latent)\n\n    # Pass fake images through discriminator\n    # targets are set to zero for all the fake images\n    fake_targets_cat = torch.zeros(fake_images_cat.size(0), 1, device=device)\n    fake_preds_cat = discriminator_cat(fake_images_cat)\n    fake_loss_cat = F.binary_cross_entropy(fake_preds_cat, fake_targets_cat)\n    fake_score_cat = torch.mean(fake_preds_cat).item()\n\n    # Update discriminator weights\n    loss_cat = real_loss_cat + fake_loss_cat\n    loss_cat.backward()\n    opt_d.step()\n    return loss_cat.item(), real_score_cat, fake_score_cat","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.973036Z","iopub.execute_input":"2022-05-16T14:18:24.973910Z","iopub.status.idle":"2022-05-16T14:18:24.986040Z","shell.execute_reply.started":"2022-05-16T14:18:24.973868Z","shell.execute_reply":"2022-05-16T14:18:24.985110Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Generator Training\n\n- We generate a batch of images using the generator, pass the into the discriminator.\n\n- We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator. \n\n- We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator.\n","metadata":{}},{"cell_type":"code","source":"def train_generator_dog(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images_dog = generator_dog(latent)\n    \n    # Try to fool the discriminator\n    preds_dog = discriminator_dog(fake_images_dog)\n    targets_dog = torch.ones(batch_size, 1, device=device)\n    loss_dog = F.binary_cross_entropy(preds_dog, targets_dog)\n    \n    # Update generator weights\n    loss_dog.backward()\n    opt_g.step()\n    \n    return loss_dog.item()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:24.989470Z","iopub.execute_input":"2022-05-16T14:18:24.990067Z","iopub.status.idle":"2022-05-16T14:18:24.998537Z","shell.execute_reply.started":"2022-05-16T14:18:24.990036Z","shell.execute_reply":"2022-05-16T14:18:24.997663Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def train_generator_cat(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images_cat = generator_cat(latent)\n    \n    # Try to fool the discriminator\n    preds_cat = discriminator_cat(fake_images_cat)\n    targets_cat = torch.ones(batch_size, 1, device=device)\n    loss_cat = F.binary_cross_entropy(preds_cat, targets_cat)\n    \n    # Update generator weights\n    loss_cat.backward()\n    opt_g.step()\n    \n    return loss_cat.item()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.001694Z","iopub.execute_input":"2022-05-16T14:18:25.002036Z","iopub.status.idle":"2022-05-16T14:18:25.010226Z","shell.execute_reply.started":"2022-05-16T14:18:25.002005Z","shell.execute_reply":"2022-05-16T14:18:25.009277Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Saving the intermediate outputs from the generator to understand the speed of the training.","metadata":{}},{"cell_type":"code","source":"sample_dir_dog = 'generated_dog'\nos.makedirs(sample_dir_dog, exist_ok=True)\nsample_dir_cat = 'generated_cat'\nos.makedirs(sample_dir_cat, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.011651Z","iopub.execute_input":"2022-05-16T14:18:25.012476Z","iopub.status.idle":"2022-05-16T14:18:25.020636Z","shell.execute_reply.started":"2022-05-16T14:18:25.012435Z","shell.execute_reply":"2022-05-16T14:18:25.019896Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def save_samples_dog(index, latent_tensors, show=True):\n    fake_images_dog = generator_dog(latent_tensors)\n    fake_fname_dog = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images_dog), os.path.join(sample_dir_dog, fake_fname_dog), nrow=8)\n    print('Saving', fake_fname_dog)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images_dog.cpu().detach(), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.021932Z","iopub.execute_input":"2022-05-16T14:18:25.022666Z","iopub.status.idle":"2022-05-16T14:18:25.031456Z","shell.execute_reply.started":"2022-05-16T14:18:25.022543Z","shell.execute_reply":"2022-05-16T14:18:25.030628Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def save_samples_cat(index, latent_tensors, show=True):\n    fake_images_cat = generator_cat(latent_tensors)\n    fake_fname_cat = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images_cat), os.path.join(sample_dir_cat, fake_fname_cat), nrow=8)\n    print('Saving', fake_fname_cat)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images_cat.cpu().detach(), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.033184Z","iopub.execute_input":"2022-05-16T14:18:25.033880Z","iopub.status.idle":"2022-05-16T14:18:25.042253Z","shell.execute_reply.started":"2022-05-16T14:18:25.033839Z","shell.execute_reply":"2022-05-16T14:18:25.041451Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model.","metadata":{}},{"cell_type":"code","source":"# creating a set of latent tensors that we can use after each epoch\nfixed_latent = torch.randn(64, latent_size, 1, 1, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.043939Z","iopub.execute_input":"2022-05-16T14:18:25.044454Z","iopub.status.idle":"2022-05-16T14:18:25.051069Z","shell.execute_reply.started":"2022-05-16T14:18:25.044403Z","shell.execute_reply":"2022-05-16T14:18:25.050128Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#saving the samples before training\nsave_samples_dog(0, fixed_latent)\nsave_samples_cat(0, fixed_latent)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.052846Z","iopub.execute_input":"2022-05-16T14:18:25.053164Z","iopub.status.idle":"2022-05-16T14:18:25.763243Z","shell.execute_reply.started":"2022-05-16T14:18:25.053102Z","shell.execute_reply":"2022-05-16T14:18:25.762528Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def fit_dog(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache() #to remove unused data from GPU  \n    \n    # Losses & scores\n    losses_g_dog = [] # generator losses\n    losses_d_dog = [] # discriminator losses\n    real_scores_dog = []\n    fake_scores_dog = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator_dog.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator_dog.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_images, _ in tqdm(train_dl_dog):\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator_dog(real_images, opt_d)\n            # Train generator\n            loss_g = train_generator_dog(opt_g)\n            \n        # Record losses & scores\n        losses_g_dog.append(loss_g)\n        losses_d_dog.append(loss_d)\n        real_scores_dog.append(real_score)\n        fake_scores_dog.append(fake_score)\n        \n        # Log losses & scores (last batch)\n        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n        # Save generated images\n        save_samples_dog(epoch+start_idx, fixed_latent, show=False)\n    \n    return losses_g_dog, losses_d_dog, real_scores_dog, fake_scores_dog","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.764946Z","iopub.execute_input":"2022-05-16T14:18:25.765434Z","iopub.status.idle":"2022-05-16T14:18:25.777262Z","shell.execute_reply.started":"2022-05-16T14:18:25.765387Z","shell.execute_reply":"2022-05-16T14:18:25.776352Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def fit_cat(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache() #to remove unused data from GPU  \n    \n    # Losses & scores\n    losses_g_cat = [] # generator losses\n    losses_d_cat = [] # discriminator losses\n    real_scores_cat = []\n    fake_scores_cat = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator_cat.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator_cat.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_images, _ in tqdm(train_dl_cat):\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator_cat(real_images, opt_d)\n            # Train generator\n            loss_g = train_generator_cat(opt_g)\n            \n        # Record losses & scores\n        losses_g_cat.append(loss_g)\n        losses_d_cat.append(loss_d)\n        real_scores_cat.append(real_score)\n        fake_scores_cat.append(fake_score)\n        \n        # Log losses & scores (last batch)\n        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n        # Save generated images\n        save_samples_cat(epoch+start_idx, fixed_latent, show=False)\n    \n    return losses_g_cat, losses_d_cat, real_scores_cat, fake_scores_cat","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.779822Z","iopub.execute_input":"2022-05-16T14:18:25.780479Z","iopub.status.idle":"2022-05-16T14:18:25.791465Z","shell.execute_reply.started":"2022-05-16T14:18:25.780436Z","shell.execute_reply":"2022-05-16T14:18:25.790661Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"lr = 0.0002\nepochs = 1000","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.793202Z","iopub.execute_input":"2022-05-16T14:18:25.793808Z","iopub.status.idle":"2022-05-16T14:18:25.804418Z","shell.execute_reply.started":"2022-05-16T14:18:25.793767Z","shell.execute_reply":"2022-05-16T14:18:25.803643Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"history_dog = fit_dog(epochs, lr)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:25.805979Z","iopub.execute_input":"2022-05-16T14:18:25.806539Z","iopub.status.idle":"2022-05-16T14:18:59.854995Z","shell.execute_reply.started":"2022-05-16T14:18:25.806495Z","shell.execute_reply":"2022-05-16T14:18:59.853533Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"history_cat = fit_cat(epochs, lr)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:59.856462Z","iopub.status.idle":"2022-05-16T14:18:59.857281Z","shell.execute_reply.started":"2022-05-16T14:18:59.857006Z","shell.execute_reply":"2022-05-16T14:18:59.857035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses_g_dog, losses_d_dog, real_scores_dog, fake_scores_dog = history_dog","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:59.858845Z","iopub.status.idle":"2022-05-16T14:18:59.859836Z","shell.execute_reply.started":"2022-05-16T14:18:59.859560Z","shell.execute_reply":"2022-05-16T14:18:59.859588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses_g_cat, losses_d_cat, real_scores_cat, fake_scores_cat = history_cat","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:59.861003Z","iopub.status.idle":"2022-05-16T14:18:59.861972Z","shell.execute_reply.started":"2022-05-16T14:18:59.861703Z","shell.execute_reply":"2022-05-16T14:18:59.861732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the checkpoints","metadata":{}},{"cell_type":"code","source":"# Save the model checkpoints \ntorch.save(generator_dog.state_dict(), 'G_dog.pth')\ntorch.save(discriminator_dog.state_dict(), 'D_dog.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:59.863159Z","iopub.status.idle":"2022-05-16T14:18:59.864162Z","shell.execute_reply.started":"2022-05-16T14:18:59.863885Z","shell.execute_reply":"2022-05-16T14:18:59.863914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model checkpoints \ntorch.save(generator_cat.state_dict(), 'G_cat.pth')\ntorch.save(discriminator_cat.state_dict(), 'D_cat.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:18:59.865381Z","iopub.status.idle":"2022-05-16T14:18:59.866253Z","shell.execute_reply.started":"2022-05-16T14:18:59.865978Z","shell.execute_reply":"2022-05-16T14:18:59.866007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}